{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 2023年10月6日开始重构\n",
    "\n",
    "- 1.akshare有一个接口更新了名字\n",
    "- 2.券商研报的表头更新了\n",
    "- 3.通过萝卜投研爬取10年pe平均值做了线程池优化（12个workers让本来50分钟的爬取减少到5分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python=3.10\n",
    "# coding=utf-8\n",
    "# author: KarlQu\n",
    "# Version: 1.1\n",
    "# Date: 2023-10-6\n",
    "# webdriver=Firefox2023.04\n",
    "# pip install akshare -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com  --upgrade\n",
    "#  mamba install -c conda-forge selenium=4.13.0\n",
    "# selenium 4 与 selenium 3 对对象选择语法做了修改\n",
    "#  mamba install -c conda-forge jupyter\n",
    "# akshare.__version__ = '1.11.23 '\n",
    "# !最重要的是g和g的可达性\n",
    "# akshare更新非常频繁\n",
    "# pip install akshare --upgrade -i https://pypi.org/simple\n",
    "\n",
    "# 是否需要更新10年pe均值和券商研报, pe_ave_10(ifupdate = False), 输出位置 f'./backups/{DATE}/nets_end.xlsx'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import akshare as ak\n",
    "import tqdm\n",
    "import time\n",
    "import datetime as dt\n",
    "import os\n",
    "import calendar\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.support.wait import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.by import By\n",
    "from concurrent import futures\n",
    "\n",
    "DATE = dt.date.today() # str(DATE) ; dt.date.today().year\n",
    "\n",
    "UA = {\n",
    "    'User-Agent':\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) Gecko/20100101 Firefox/105.0'\n",
    "}\n",
    "\n",
    "def open_file(file_name:str, upperdir = 'backups'):\n",
    "    \"\"\"创建文件夹\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : 日期字符串\n",
    "    upperdir : 上级目录名\n",
    "    \"\"\"\n",
    "    if os.path.exists(f'./{upperdir}/' + file_name):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(f'./{upperdir}/' + file_name)\n",
    "\n",
    "def update_g(ifupdate = False) -> pd.DataFrame:\n",
    "    \"\"\"akshare的个股研报接口\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ifupdate : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        columns = ['代码','名称','研报数','2021预测每股收益','2022预测每股收益','2023预测每股收益','2024预测每股收益']\n",
    "        \n",
    "        共4年的每股收益,2023年更新为'2022预测每股收益','2023预测每股收益','2024预测每股收益','2025预测每股收益'\n",
    "    \"\"\"\n",
    "    if ifupdate:\n",
    "        open_file(str(DATE), upperdir = 'backups')\n",
    "        profit_forecast = ak.stock_profit_forecast_em().iloc[:,[1,2,3,9,10,11,12]].drop_duplicates()\n",
    "        profit_forecast.to_excel(f'./backups/{DATE}/profit_forecast.xlsx',index=False)\n",
    "\n",
    "    else:\n",
    "        latest_file = list(os.walk('./backups/'))[0][1][-1]\n",
    "        profit_forecast = pd.read_excel(f'./backups/{latest_file}/profit_forecast.xlsx',dtype={'代码':'str'})\n",
    "    # 调整g,收益>0、每期g>10%、每期g可信值35%。 得到调整的几何均数g\n",
    "    profit_forecast = profit_forecast[(profit_forecast['2022预测每股收益']>0) & (profit_forecast['2023预测每股收益']>0 )& ( profit_forecast['2024预测每股收益']>0)]\n",
    "    profit_forecast\n",
    "    profit_forecast1 = profit_forecast.iloc[:,[3,4,5,6]].pct_change(axis=1)[['2023预测每股收益','2024预测每股收益',\t'2025预测每股收益']]\n",
    "    profit_forecast1\n",
    "    profit_forecast2 = profit_forecast1[(profit_forecast1['2023预测每股收益']>0.1) & (profit_forecast1['2024预测每股收益']>0.1 )& ( profit_forecast1['2025预测每股收益']>0.1)]\n",
    "    profit_forecast2\n",
    "    profit_forecast2[profit_forecast2>0.35] = 0.35\n",
    "    profit_forecast3 = profit_forecast2 + 1\n",
    "    g_scaled = ((profit_forecast3['2023预测每股收益']*profit_forecast3['2024预测每股收益']*profit_forecast3['2025预测每股收益'])**(1/3)-1)\n",
    "    g_scaled.name = 'g'\n",
    "    g = pd.concat([g_scaled,profit_forecast],axis=1,join='inner')[['代码','名称','g','研报数']].reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "\n",
    "def ten_year_ago(date: dt.date = DATE) -> str:\n",
    "        \"\"\"10年前日期，月底对应月底\n",
    "        Parameters\n",
    "        ----------\n",
    "        date : datetime.date\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "        \"\"\"\n",
    "        year = date.year - 10\n",
    "        day = min(date.day, calendar.monthrange(year, date.month)[1])\n",
    "        dt = date.replace(year=year, month=date.month, day=day)\n",
    "        # print(dt)\n",
    "        bigen_date = str(dt).replace('-','')\n",
    "        return bigen_date\n",
    "\n",
    "def pe_ave_10(date:str = str(DATE), ifupdate = False):\n",
    "    \"\"\"获取or更新10年pe均值和券商研报的预期每股收益\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : str, optional\n",
    "        _description_, by default str(DATE)\n",
    "    ifupdate : bool, optional\n",
    "        _description_, by default False\n",
    "    \"\"\"\n",
    "    if ifupdate:\n",
    "        g = update_g(ifupdate=ifupdate)\n",
    "        stock_ids = g.代码.to_list() # 每次爬pe历史均值的时候都更新一下券商研报\n",
    "        print('代码前五个')\n",
    "        print(stock_ids[0:5])\n",
    "        # 1. selenium登录获取cookies\n",
    "        denglu_link = 'https://robo.datayes.com/v2/landing/peband'\n",
    "        cruser = webdriver.Firefox()\n",
    "        cruser.get(denglu_link)\n",
    "        input(\"扫码登录后按回车键\")\n",
    "        # WebDriverWait(cruser,60).until(EC.presence_of_all_elements_located((By.ID, 'app')))\n",
    "        cookies = {x['name']:x['value'] for x in cruser.get_cookies()}\n",
    "        print(cookies)\n",
    "        end_date = date.replace('-','')\n",
    "        begin_date = ten_year_ago()\n",
    "        ave_pettm = []\n",
    "        executor = futures.ThreadPoolExecutor(max_workers=12)\n",
    "        fs = []\n",
    "        total_tasks = len(stock_ids)\n",
    "        with tqdm.tqdm(total=total_tasks) as pbar:\n",
    "            for ids in stock_ids:\n",
    "                def get_ids_peave(ids, begin_date, end_date, cookies, UA, ave_pettm):\n",
    "                    try:\n",
    "                        url = f'https://gw.datayes.com/rrp_adventure/web/stockModel/band/{ids}' # 2022old\n",
    "                        response = requests.get(url=url,\n",
    "                                                params= {\n",
    "                                                    'apiType': '4',\n",
    "                                                    'category': '1',    # 1为PE，2为PB\n",
    "                                                    'subCategory': '1',\n",
    "                                                    'flag': '-1',\n",
    "                                                    'beginDate': f'{begin_date}',\n",
    "                                                    'endDate': f'{end_date}'\n",
    "                                                },\n",
    "                                                cookies=cookies,\n",
    "                                                headers=UA)\n",
    "                        ave_pettm.append([ids, response.json()['data']['mean']])\n",
    "                        pbar.update(1) # 更新tqdm进度条用\n",
    "                        time.sleep(np.random.rand()) # 1536/1536 [33:05<00:00,  1.29s/it]\n",
    "                        # print(f'正常{response.status_code} , code: {ids}')\n",
    "                    except:\n",
    "                        print(f'异常{response.status_code} , code: {ids}')\n",
    "                        pass\n",
    "\n",
    "                f= executor.submit(get_ids_peave, ids, begin_date, end_date, cookies, UA, ave_pettm)\n",
    "                fs.append(f)\n",
    "            futures.wait(fs)\n",
    "        cruser.quit()\n",
    "        # 3. 本地保存\n",
    "        pe_ave = pd.DataFrame(ave_pettm, columns=['code','ave_pettm'])\n",
    "        open_file(str(DATE), upperdir = 'backups')\n",
    "        pe_ave.to_excel(f'./backups/{DATE}/avepettm_backup.xlsx',index=False)\n",
    "        latest_file = DATE\n",
    "    else:\n",
    "        latest_file = list(os.walk('./backups/'))[0][1][-1]\n",
    "        print(f'研报和10年期pe平均值的最近一次更新时间为 {latest_file}')\n",
    "        g = update_g(ifupdate=ifupdate)\n",
    "        pe_ave = pd.read_excel(f'./backups/{latest_file}/avepettm_backup.xlsx',dtype={'code':'str'})\n",
    "    nets = pd.merge(g.rename(columns={'代码':'code'}) , pe_ave , on='code',how='left')\n",
    "    nets['peg'] = (nets.ave_pettm / (100*nets.g))\n",
    "    nets1 = nets[(nets['peg']<1.2)]\n",
    "    # (nets['peg']<1.2).sum()\n",
    "    # 2. 得到筛选后的股票代码，后续可以爬取pettm和dvttm了\n",
    "    stock_ids_selected = nets1.code.to_list()\n",
    "    print('DEBUG:print 代码前五')\n",
    "    print(stock_ids_selected[0:5])\n",
    "    pe_ttm = []\n",
    "    for ids in tqdm.tqdm(stock_ids_selected):\n",
    "        try:\n",
    "            rrr = ak.stock_a_indicator_lg(symbol=f'{ids}').iloc[-1,[0,2,7]] # pettm and dvttm 接口\n",
    "            rrr['industry'] = ak.stock_individual_info_em(symbol=f\"{ids}\").loc[2].value # 行业接口\n",
    "            rrr.name = ids\n",
    "            pe_ttm.append(rrr)\n",
    "        except:\n",
    "            print(f'{ids} 查无!')\n",
    "        continue\n",
    "\n",
    "    pe_and_dv = pd.concat(pe_ttm,axis=1).T\n",
    "    pe_and_dv_backup = pe_and_dv.reset_index(drop=False)\n",
    "    pe_and_dv_backup = pe_and_dv_backup.rename(columns={'index':'code'}) \n",
    "    open_file(str(DATE), upperdir = f'backups/{latest_file}')\n",
    "    pe_and_dv_backup.to_excel(f'./backups/{latest_file}/{DATE}/pe_and_dv.xlsx',index=False)\n",
    "    # pe_ttm = pd.read_excel(f'./backups/{latest_file}/{DATE}/pe_and_dv.xlsx',dtype={'code':'str'})\n",
    "    nets2 = pd.merge(nets1,pe_and_dv_backup,on='code',how='left')\n",
    "    nets3 = nets2[nets2.pe_ttm<nets2.ave_pettm]\n",
    "    nets3['peg'] = (nets3.pe_ttm / (100*nets3.g))\n",
    "    nets3['exp_reward_yearly'] = 100*((nets3.ave_pettm/nets3.pe_ttm)**(1/3)*(1+nets3.g)-1)\n",
    "    nets_end = nets3[nets3.exp_reward_yearly>35].drop_duplicates()\n",
    "    nets_end.to_excel(f'./backups/{latest_file}/{DATE}/nets_end.xlsx',index=False)   \n",
    "    print('done! open nets_end.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近一次更新时间为 2023-10-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opink\\AppData\\Local\\Temp\\ipykernel_7152\\4135317224.py:78: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Call ffill before calling pct_change to retain current behavior and silence this warning.\n",
      "  profit_forecast1 = profit_forecast.iloc[:,[3,4,5,6]].pct_change(axis=1)[['2023预测每股收益','2024预测每股收益',\t'2025预测每股收益']]\n",
      "C:\\Users\\opink\\AppData\\Local\\Temp\\ipykernel_7152\\4135317224.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  profit_forecast2[profit_forecast2>0.35] = 0.35\n",
      "C:\\Users\\opink\\AppData\\Local\\Temp\\ipykernel_7152\\4135317224.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  profit_forecast2[profit_forecast2>0.35] = 0.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码前五个\n",
      "['603369', '603801', '002867', '002142', '002271']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 342/439 [09:13<12:26,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605277 查无!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 408/439 [10:52<00:49,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832469 查无!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [11:39<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! open nets_end.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opink\\AppData\\Local\\Temp\\ipykernel_7152\\4135317224.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nets3['peg'] = (nets3.pe_ttm / (100*nets3.g))\n",
      "C:\\Users\\opink\\AppData\\Local\\Temp\\ipykernel_7152\\4135317224.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nets3['exp_reward_yearly'] = 100*((nets3.ave_pettm/nets3.pe_ttm)**(1/3)*(1+nets3.g)-1)\n"
     ]
    }
   ],
   "source": [
    "# pe_ave_10(ifupdate=True)\n",
    "pe_ave_10(ifupdate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AkShare的legulegu.com/robots.txt协议对爬虫没做限制,但是！？\n",
    "尝试也改造成线程池加速\n",
    "> 被封ip了。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('./backups/', ['2022-10-16', '2023-10-06'], []),\n",
       " ('./backups/2022-10-16',\n",
       "  [],\n",
       "  ['avepettm_backup.xlsx',\n",
       "   'nets_end.xlsx',\n",
       "   'pe_and_dv.xlsx',\n",
       "   'profit_forecast.xlsx']),\n",
       " ('./backups/2023-10-06',\n",
       "  [],\n",
       "  ['avepettm_backup.xlsx',\n",
       "   'nets_end.xlsx',\n",
       "   'pe_and_dv.xlsx',\n",
       "   'profit_forecast.xlsx'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "list(os.walk('./backups/')) # [0][1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firefox202304geckodriver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
